{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement CNN from Page 496 (Chapter14) for the Fashion MNIST dataset (Chapter 10, Page 318) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVTOP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Testen, ob TensorFlow korrekt installiert a\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available CPUs:\", tf.config.list_physical_devices('CPU'))\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes memory issue\n",
    "# Before this: compiled model reserves nearly all of the GDDR (7 of 8 GB GDDR) -> crashed\n",
    "# This fixes memory usage to about 20 %\n",
    "# Here memory is only reserved when needed (dynamic)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load famous fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# Already shuffled and split into training set (60k images) and test set (10k images)\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "# Hold out the last 5k images from training set for validation\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras images are represented by 28x28 rather than 1-D 784 (sci-kit learn)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras pixel intensity integers 0-255, rather than float 0.0-255.0 (sci-kit learn)\n",
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplicity: Scale down to 0-1 range and transform to float by dividing by 255.0\n",
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add greyscale dimension to fit into expected model input\n",
    "# Alternatives, np.reshape, np.expanddims, or Reshape layer in CNN\n",
    "X_train, X_valid, X_test = X_train[:,:,:, np.newaxis], X_valid[:,:,:, np.newaxis], X_test[:,:,:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension now [28, 28, 1]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect output to classes defined in \"https://keras.io/api/datasets/fashion_mnist/\"\n",
    "class_names = [\"T-shirt/top\", \"Trousers\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train contains 4,9,0,2 class labels\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN\n",
    "- Page 496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial introduced in Chapter 11\n",
    "from functools import partial\n",
    "\n",
    "# Acts like Conv2D but with different default arguments:\n",
    "# small kernel size of 3\n",
    "# same padding\n",
    "# ReLu activation function and corresponding He initializer\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Start with a large filter (7x7)\n",
    "    # Default stride of 1 , because images small\n",
    "    # Input shape according to dataset with a single (greyscale) color (maybe use Reshape layer)\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28,28,1]),\n",
    "    # MaxPooling layer with default stride of 2, therefore each dimension divided by 2\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    ### Repeat 2x : 2x CNN + MaxPooling\n",
    "    # For larger images this could be repeated further\n",
    "    # Note: \n",
    "    #   Filters double after MaxPooling as shape reduces by half, no fear: exploding parameters, memory usage, computational load\n",
    "    #   Filters get larger towards the output layer: 64, 128, 256\n",
    "    #   Low-level features are mostly low (small circles, horizontal lines)\n",
    "    #   But many ways to combine them into higher-level features (e.g. face)\n",
    "    # 1.\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    # 2.\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    # Fully connected layer\n",
    "    # Flatten -> 2-D image to 1-D array\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Dense: each node is connected with all of the others\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    # Each training iteration a random subset of all neurons in one layer (except output layer) are dropped out (output 0)\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Fewer units towards the output layer\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    # Dropout 50 % still\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Softmax converts into propabilities\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "            # Sparse labels: i.e. for each instance, there is just a target class index from 0 to 9\n",
    "            # One-hot vector like output would be just categorical_crossentropy\n",
    "            # Binary or multilabel binary classification: sigmoid in output instead of softmax + binary_crossentropy \n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"sgd\", # Gradient decent?\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking \n",
    "run_num = \"1\"\n",
    "graph_dir = f\"logs/{run_num}/trace\"\n",
    "graph_name = f\"trace_{run_num}\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/{run_num}\", histogram_freq=1, write_graph=True)\n",
    "\n",
    "# Retrieve GPU Usage after each epoch\n",
    "import subprocess\n",
    "class GPUMonitorCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        result = subprocess.run([\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"], \n",
    "                                stdout=subprocess.PIPE)\n",
    "        gpu_info = result.stdout.decode('utf-8').strip()\n",
    "        print(f\"Epoch {epoch + 1} GPU usage: {gpu_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set: actual data model trains on\n",
    "# Test set: \n",
    "#   ...is independent of the training set but has a somewhat similar type of probability distribution of classes \n",
    "#   and is used as a benchmark to evaluate the model, used only after the training of the model is complete. \n",
    "# Validation set:\n",
    "#   ...is used to fine-tune the hyperparameters of the model and is considered a part of the training of the model.\n",
    "# Default batch size 32\n",
    "\n",
    "tf.summary.trace_on(graph=True, profiler=True,profiler_outdir=graph_dir)\n",
    "# Dann deinen Trainingscode ausführen\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, GPUMonitorCallback()])\n",
    "# Anschließend exportieren:\n",
    "with tf.summary.create_file_writer(graph_dir).as_default():\n",
    "    tf.summary.trace_export(name=graph_name, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs/1/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
